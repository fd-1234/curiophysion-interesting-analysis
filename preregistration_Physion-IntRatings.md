# Pregistration for Physion-IntRatings

**Researchers**: Felix, Sama, curiophysion team

## Study information
**Title**: Physion-IntRatings (explicit interestingness tags on Physion stimuli)

### Research questions
In this study, we aim to identify features of dynamic physical scenes that make them more or less interesting, and to investigate if these 'interestingness' scores correlate to scenes that were more or less difficult for human participants or computational models to predict correctly on the Physion Object Contact Prediction (OCP) task.

### Hypotheses
<!-- list 2 specific, concise, and testable hypotheses, including the if-then logic statements for your predictions. -->
Insofar as interestingness ratings reflect learnable difficulty for improving physical prediction, then we predict that:
* Scenes that were the easiest to predict would be rated as being the least interesting. 
* Scenes that were relatively difficult to predict (near-chance accuracy on the object-contact prediction task in the original Physion experiments) will generally be rated as more interesting. 
* Scenes that were especially difficult for people to accurately predict (below-chance accuracy on the object-contact prediction task in the original Physion experiments), we predict that these will only be rated as more interesting when the full-length video clip is shown, but less interesting when only the initial 1500ms clip (pre-contact) is shown.

<!-- Also: camera angle, jitter, percent of target visible (_id map) -->
 
## Design Plan
###   Study type
 <!-- indicate whether your study will be experimental or correlational -->
 Experimental
###   Study design
 <!-- describe the overall design of the study (what will be manipulated and/or measured, specify whether manipulations will be between- or within-participants, etc.) -->
We will conduct 16 experiments (8 with short-length videos [OCP task stimuli] not showing the outcome of the physical scenario and 8 with full-length videos showing the outcome), each testing interestingness judgments for the Physion categories: collide, contain, dominos, drape, drop, link, roll, support.

We will ask participants to rate "How interesting is this interaction?" on a numbered 5-point Likert scale, with only endpoints labelled "not at all interesting" and "extremely interesting". 

These Physion scenarios were generated by sampling values of various physical parameters (e.g., number of physical elements, number of occluder objects, positional jitter, etc.) and generating a stimulus set containing about 150 example scenes. From this set of 150 scenarios, about 50% of the chosen scenes are positive trials (ie. the red target object touches the yellow target zone) and 50% are negative trials (ie. the red target object does not come in contact with the the yellow target zone.

####   Manipulated variables
 <!-- If applicable, precisely define any variables you plan to manipulate, including the levels and whether the manipulation will be between or within participants. -->
As outlined above, participants are not assigned to any conditions.

###   Study design: evaluation protocol 

**Sequence of events in a session**
1. Consent form and study information
2. Task explanation
3. Familiarization trials â€“ 2 shown
		1. First frozen frame shown for 2000ms, with red/yellow segmentation map indicating agent/patient object flashing at 2Hz
    2. Video is played for 1500ms (short-length video version) or 10000 ms or length of video, whichever is shorter (full-length video version), then hidden
    3. Participants can proceed after full video has played
4. Comprehension check <!-->Might be moved to the end of the session<-->
5. Participants are informed that the full experiment starts
6. 150 trials
    1. Fixation cross is shown for random interval between 500ms and 1500ms
    2. First frozen frame shown for 2000ms, with red/yellow segmentation map indicating agent/patient object flashing at 2Hz
    3. Video is played for 1500ms (short-length video version) or 10000 ms or length of video, whichever is shorter (full-length video version), then hidden
    4. Interestingness rating is queried from subject (5-point Likert scale)
7. Demographics & Feedback
    * age
    * gender
    * education level
    * difficulty rating ("How difficult did you find this task?", 5 point Likert scale)
9. End of study

We have two sets of stimuli, containing a video clip of a visual scene contantaining various objects physical interacting with each other. One task version will ask for interestingness ratings for the the short-length video clips used in the Physion OCP task, where the outcome of the prediction task is not shown. The second version will ask for interestiness ratings for the full-length video clips, where the outcome (red target object and yellow target zone either make contact or no-contact) is shown.

Each of these 150 trials began with a fixation cross, which was shown for a randomly sampled time between 500ms and 1500ms.
To indicate which of the objects shown is the agent and patient object, participants were then shown the first frame of the video for 2000ms. During this time, the agent and patient objects were overlaid in red and yellow respectively. The overlay flashed on and off with a frequency of 2Hz. 
After this, the first 1500ms of the stimulus were played (in short-length version) or the entire video was played for xxxx ms (full-lenth version). After completion of video clip, the stimulus is removed and the response buttons are enabled. The experiments moved to the next phase after the participants made a interestingness rating by selecting a score from 1 to 5 on the Likert scale.

Participants first completed 2 familiarization trials before moving on to complete 150 test trials. 
During the familiarization phase, all participants were presented with one positive (ie., the red target object touches the yellow target zone) and one negative trial (ie., there is no contact between red target object and yellow target zone) . 

During the test phase, participants were presented with the same set of stimuli in a randomized sequence.

###   Measured variables
 <!-- Precisely define each variable that you will measure. This includes outcome measures, as well as other measured predictor variables. -->
We measure:
* `response`: interestiness rating (1 to 5)
* `rt`: time taken to make prediction

 After the trials, participants will be asked to provide:
 * age
 * gender
 * education level
 * difficulty rating ("How difficult did you find this task?", 5-point Likert scale)
 * free form feedback on the task

## Sampling Plan
###   Data collection procedure
 <!-- describe the method you will use to collect your data, and your inclusion/exclusion criteria. This should include your sampling frame, how participants will be recruited, and whether/how they will be compensated. -->
Participants will be recruited from Prolific and compensated at least $14/hr. 
Because the length of the experiment varies depending on the video length condition, the total compensation differs between conditions.
 Participants will not be rewarded for performance.

Participants are only allowed to take the task once. 
Participants are only allowed to the task in one of the two video length conditions.
However, participants are able to take a version of the experiment with another scenario.

###   Sampling procedure
 <!-- indicate your target sample size and why that is your target (might be based in past research, for example) -->

 <!-- ###   Stopping rule -->
 <!-- specify how you will determine when to stop data collection -->
 <!-- Data collection will be stopped after the planned number of participants has been recorded. -->
 Data collection will be stopped after 100 participants have completed the experiment. 

## Analysis Plan
###   Data exclusion (finalized before beginning formal analyses on Month, 2021)
 <!-- How will you determine which data points or samples (if any) to exclude from your analyses? How will outliers be handled? Will you use any awareness or attention check? -->
 Data from an entire experimental session will be excluded if the responses:
 * Participants who do not complete all trials
 * Partcipants whose exit survey indicates they did not undersatnd the study (self-reported)
 * have a distribution of responses where >80% of responses are of a single button
 * fail to include at least one response for each of the endpoints of the scale (1 & 5)
 * the mean log-transformed response time for that participant is 3 standard deviations above the median log-transformed response time across all participants for that scenario

Additionally, a comprehension check question will be asked after the familiarization phase, but before the test phase. Participants will be asked a multiple choice question how they're going to rate the upcoming videos. 
If they fail, they will shown the instruction again and asked to review them carefully. Their responses will be flagged.
 
Excluded sessions will be flagged (incomplete sessions will be excluded outright, however). Flagged sessions will not be included in the main analyses. We will also conduct our planned analyses with the flagged sessions included to investigate the extent to which the outcomes of the main analyses change when these sessions are included. Specifically, we will fit a statistical model to all sessions and estimate the effect of a session being flagged on accuracy. 
 
###  Missing data
We will only include sessions that are complete (i.e., response collected for all trials) in our main analyses.

### Planned analyses

#### Human interestingness ratings across participants for each stimulus
We will analyze interestingess score for each stimulus within scenario by computing the average rating of all participants who viewed that stimulus. 
Additionally, we will compute the standard deviation of the ratings across all participants for each stimulus to identify stimuli for which interestingness ratings diverge.

#### Correlation between human interestingness ratings and human accuracy per stimulus
In order to test the hypothesis that interestingness is driven by poor predictability of a stimulus, we will compute the correlation between human interestingness ratings and human accuracy on the OCP task for each stimulus.

Specifically, we aim to test the following regression models:
* `interestingness ~ accuracy`
* `interestingness ~ accuracy^2`

Additionally, we will compute Spearman's rank correlation $\rho$ between human interestingness ratings and human accuracy on the OCP task for each stimulus.

The relationship will be visualized by plotting each stimulus on a scatter plot with the average accuracy on the x-axis and average interestingness on the y-axis.

#### Correlation between human interestingness ratings by scenario and human accuracy by scenario
In order to test the hypothesis that interestingness is driven by poor predictability of a scenario, we will compute the correlation between human average interestingness ratings and average human accuracy on the OCP task for each scenario.

Specifically, we aim to test the following regression models:
* `interestingness ~ accuracy`
* `interestingness ~ accuracy^2`

Additionally, we will compute Spearman's rank correlation $\rho$ between human interestingness ratings and human accuracy on the OCP task for each scenario.

The relationship will be visualized by plotting each stimulus on a scatter plot with the average accuracy on the x-axis and average interestingness on the y-axis.

#### Human-model comparisons

### Supplemental analyses

#### Human interestingness ratings across scenario
We will analyze interestingness score for each scenario by computing the ratings across all stimuli. 

#### Correlation between human interestingness across scenario and human accuracy across scenario
<!-- How would that look? -->
